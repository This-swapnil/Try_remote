# LLM-RAB-WebUI-integration
[ollama](https://ollama.com/) 
[ollama repo](https://github.com/ollama/ollama)
Must be able to run program on the terminal of your machine. Talk to the program so you need a Voice Activity Detector (webrtcvad, sileroVA etc.), 
a speech to text transcriber model (i.e whisper, faster-whisper etc.), and a text-to-speech library for the program to talk back as a bonus (the library doesnt have to be great, again this is a bonus). 

mode for Retreival Augmented Generation (RAG) using ollama. There are some example tutorials in the ollama repo. You sould be able to send website links for the program to perform RAG on. For this you can paste the website link on the terminal to provide input.

streamlit and Flask. User bubble and LLM response bubbles must be different color.

LLMs that are small i.e gemma:2b, tiny-llama etc.

If PC is good (extra points) mode for multi-modal input (llava:7b).
